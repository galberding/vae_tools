{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version: 2.2.4\n",
      "tensorflow version: 1.12.0\n",
      "matplotlib uses:  module://ipykernel.pylab.backend_inline\n",
      "Available GPUs ['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.stats import norm\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\n",
    "from keras.layers import Conv1D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import keras\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "import sys, os\n",
    "from vae_tools import plot_model, layers, nb_tools, viz, loader, build_model, sanity, sampling, custom_variational_layer, mmvae\n",
    "import vae_tools\n",
    "import vae_tools.callbacks\n",
    "sanity.check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "steps_around_center, angles_around_center = loader.get_steps_around_hokuyo_center(degree_around_center = 80.)\n",
    "image_rows_cols_chns = (64, 64, 2)\n",
    "image_original_rows_cols_chns= (800,800,3)\n",
    "new_shape = image_rows_cols_chns\n",
    "old_shape = image_original_rows_cols_chns\n",
    "measurements_per_file = 111 # 2018-06-02 dataset\n",
    "measurements_per_file = 300 # 2018-06-05 dataset\n",
    "# folder = glob('2018-06-02/cyl_r*/') + glob('2018-06-02/box_r*/');\n",
    "# folder = glob('2018-06-05/cyl_r*/') + glob('2018-06-05/box_r*/') + glob('2018-06-05/cyl_g*/');\n",
    "folder = ('../../mVAE/2018-06-05/cyl_r*/', '../../mVAE/2018-06-05/box_r*/', '../../mVAE/2018-06-05/cyl_g*/');\n",
    "X_l, X_c, X_set_label_idx, X_set_label_str = loader.camera_lidar(\"./Xl-80deg_Xc-64-64-2\", folder, \n",
    "                    \"_amiro1_sync_front_camera_image_raw-X-pixeldata.npz\", \n",
    "                    \"_amiro1_sync_laser_scan-X-ranges_intensities_angles.npz\", \n",
    "                    measurements_per_file, old_shape, new_shape, 683, steps_around_center, 5., overwrite = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lidar_mask(X_set, original_dim, empty_perc = 1.0):\n",
    "    # Prepare mask to remove all data where the objects are to small (to avoid learning empty measurements) (100% i.e. take everything)\n",
    "    mask = np.ones((len(X_set),), dtype=np.bool)\n",
    "    for idx in np.arange(len(X_set)):\n",
    "        if np.sum(X_set[idx,:] == 0.0) / original_dim > empty_perc:\n",
    "            mask[idx] = False\n",
    "    return mask\n",
    "\n",
    "def get_camera_mask(X_set, original_dim, empty_perc = 0.70):\n",
    "    # Prepare the mask for removing all data which has a certain amount of empty space (to avoid learning empty images)\n",
    "    empty_perc = 0.70\n",
    "    mask = np.ones((len(X_set),), dtype=np.bool)\n",
    "    for idx in np.arange(len(X_set)):\n",
    "        if np.sum(X_set[idx,:] == 0.0) / original_dim > empty_perc:\n",
    "            mask[idx] = False\n",
    "    return mask\n",
    "\n",
    "def get_lidar_data(X_l, X_set_label_idx, original_dim, use_conv, mask = None, X_set_random_idx = None, X_train_size = None):\n",
    "    # Data normalization\n",
    "    X_set = np.copy(X_l)\n",
    "    X_set = X_set / np.amax(X_set)\n",
    "    X_set_label = np.copy(X_set_label_idx)\n",
    "    if mask is None:\n",
    "        mask = get_lidar_mask(X_set, original_dim, empty_perc = 1.0)\n",
    "\n",
    "    # normalization (min-0/max-1 and signal spreading)\n",
    "    if True:\n",
    "        for idx in np.arange(len(X_set)):\n",
    "            min_val = np.amin(X_set[idx,X_set[idx,:] != 0.0])\n",
    "            X_set[idx,X_set[idx,:] != 0.0] = X_set[idx,X_set[idx,:] != 0.0] - min_val\n",
    "            max_val = np.amax(X_set[idx,:])\n",
    "            X_set[idx,:] = X_set[idx,:] / max_val\n",
    "            #\n",
    "            X_set_val = X_set[idx,X_set[idx,:] != 0.0]\n",
    "            num_values = len(X_set_val)\n",
    "            idx_interp = np.linspace(0, num_values, num=227, endpoint=True) \n",
    "            idx_orig = np.linspace(0, num_values, num=num_values, endpoint=True)\n",
    "            X_set[idx,:] = np.interp(idx_interp, idx_orig, X_set_val)\n",
    "\n",
    "    # Shape for the input\n",
    "    if use_conv:\n",
    "        # remove the last value to make the number of measurements even\n",
    "        X_set = X_set[:,:-1]\n",
    "        # Add extra dimension\n",
    "        X_set = np.expand_dims(X_set, axis=-1)\n",
    "\n",
    "    # Remove unwanted the data\n",
    "    X_set = X_set[mask,:]\n",
    "    X_set_label = X_set_label[mask]\n",
    "\n",
    "    # Get training and test set\n",
    "    if X_set_random_idx is None:\n",
    "        X_set_random_idx = np.arange(X_set.shape[0])\n",
    "        np.random.shuffle(X_set_random_idx)\n",
    "    if X_train_size is None:\n",
    "        X_train_size = np.int(len(X_set) * 0.99)\n",
    "    X_train = X_set[X_set_random_idx[:X_train_size],:]\n",
    "    X_test = X_set[X_set_random_idx[X_train_size:],:]\n",
    "    X_train_label = X_set_label[X_set_random_idx[:X_train_size]]\n",
    "    X_test_label = X_set_label[X_set_random_idx[X_train_size:]]\n",
    "    # Check normalization\n",
    "    print(\"Minimum value: \" + str(np.amin(X_set)))\n",
    "    print(\"Maximum value: \" + str(np.amax(X_set)))\n",
    "    return X_train, X_test, X_train_label, X_test_label\n",
    "\n",
    "def get_camera_data(X_c, X_set_label_idx, original_dim, use_conv, mask = None, X_set_random_idx = None, X_train_size = None):\n",
    "    # Format the data for training\n",
    "    X_set_label = np.copy(X_set_label_idx)\n",
    "    X_set = np.reshape(np.copy(X_c), (len(X_c), image_rows_cols_chns[0], image_rows_cols_chns[1], image_rows_cols_chns[2]))\n",
    "    X_set = 1. - X_set # Invert the brightness\n",
    "    ## Normalize: Threshold uninformative (< 0.5 = 0), max=1\n",
    "    X_set[X_set < 0.5] = 0\n",
    "    X_set = X_set / np.amax(X_set)\n",
    "    # Copy the upper half of the image to the lower one, to fill the floor with the object as well (to get more information)\n",
    "    for idx in np.arange(len(X_set)):\n",
    "        X_set[idx,int(new_shape[0]/2):,:,:] = np.flipud(X_set[idx,:int(new_shape[0]/2),:,:])\n",
    "\n",
    "    if mask is None:\n",
    "        mask = get_camera_mask(X_set, original_dim, empty_perc = 0.70)\n",
    "\n",
    "    # Flatten the data if we do not use conv layers\n",
    "    if not use_conv:\n",
    "        X_set = X_set.reshape(-1,np.prod(image_rows_cols_chns))\n",
    "    # Check normalization\n",
    "    print(\"Minimum value: \" + str(np.amin(X_set)))\n",
    "    print(\"Maximum value: \" + str(np.amax(X_set)))\n",
    "\n",
    "    # Remove\n",
    "    X_set = X_set[mask,:]\n",
    "    X_set_label = X_set_label[mask]\n",
    "\n",
    "\n",
    "    # define train and test set\n",
    "    if X_set_random_idx is None:\n",
    "        X_set_random_idx = np.arange(X_set.shape[0])\n",
    "        np.random.shuffle(X_set_random_idx)\n",
    "    if X_train_size is None:\n",
    "        X_train_size = np.int(len(X_set) * 0.99)\n",
    "    X_train = X_set[X_set_random_idx[:X_train_size],:]\n",
    "    X_test = X_set[X_set_random_idx[X_train_size:],:]\n",
    "    X_train_label = X_set_label[X_set_random_idx[:X_train_size]]\n",
    "    X_test_label = X_set_label[X_set_random_idx[X_train_size:]]\n",
    "\n",
    "    return X_train, X_test, X_train_label, X_test_label\n",
    "\n",
    "def get_data(X_c, X_l, X_set_label_idx, original_dim_lidar, original_dim_camera, use_conv_lidar, use_conv_camera):\n",
    "    mask = np.logical_and(get_camera_mask(X_c, original_dim_camera, empty_perc = 0.70), get_lidar_mask(X_l, original_dim_lidar, empty_perc = 1.0))\n",
    "    num_samples = np.sum(mask)\n",
    "    X_set_random_idx = np.arange(num_samples) #  no randomnes\n",
    "    np.random.shuffle(X_set_random_idx)\n",
    "    X_train_size = np.int(num_samples * 0.99)\n",
    "    X_c_train, X_c_test, X_c_train_label, X_c_test_label = get_camera_data(X_c, X_set_label_idx, original_dim_camera, use_conv_camera, mask = mask, X_set_random_idx = X_set_random_idx, X_train_size = X_train_size)\n",
    "    X_l_train, X_l_test, _, _ = get_lidar_data(X_l, X_set_label_idx, original_dim_lidar, use_conv_lidar, mask = mask, X_set_random_idx = X_set_random_idx, X_train_size = X_train_size)\n",
    "    return X_l_train, X_l_test, X_c_train, X_c_test, X_c_train_label, X_c_test_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_lidar(train = True, use_conv = False, X_set = None, epochs = 500):\n",
    "    \n",
    "    # Setup the VAE\n",
    "    batch_size = 256\n",
    "    if use_conv:\n",
    "        original_dim = 226\n",
    "        # Conv settings\n",
    "        # number of convolutional filters to use\n",
    "        filters = 16\n",
    "        # convolution kernel size\n",
    "        num_conv = 5\n",
    "    else:\n",
    "        original_dim = 227\n",
    "    intermediate_dim = 128\n",
    "    original_dim_2 = int(original_dim / 2)\n",
    "    intermediate_dim_2 = int(intermediate_dim / 2)\n",
    "    latent_dim = 2\n",
    "    epsilon_std = 1.\n",
    "    beta = 0.125\n",
    "    # beta = 0.01 # works as well\n",
    "\n",
    "    if use_conv:\n",
    "        class Conv1DTranspose():\n",
    "            def __init__(self, filters, kernel_size, strides=2, activation='relu', padding='same'):\n",
    "                self.input  = x = Lambda(lambda x: K.expand_dims(x, axis=2))\n",
    "                self.conv   = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1), padding=padding)\n",
    "                self.output = Lambda(lambda x: K.squeeze(x, axis=2))\n",
    "            def __call__(self, input_tensor):\n",
    "                return self.output(self.conv(self.input(input_tensor)))\n",
    "\n",
    "        encoder = [[\n",
    "            Input(shape=(original_dim,1)),\n",
    "            Conv1D(1, kernel_size=2, padding='same', activation='relu'),\n",
    "            Conv1D(filters, kernel_size=2, padding='same', activation='relu', strides=2),\n",
    "            Conv1D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1),\n",
    "            Conv1D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1),\n",
    "            Flatten(),\n",
    "            Dense(intermediate_dim, activation='relu')\n",
    "        ]]\n",
    "\n",
    "        output_shape = (batch_size, original_dim_2, filters)\n",
    "\n",
    "        decoder = [[\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(filters * original_dim_2, activation='relu'),\n",
    "            Reshape(output_shape[1:]),\n",
    "            Conv1DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu'),\n",
    "            Conv1DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu'),\n",
    "            Conv1DTranspose(filters, kernel_size=3, strides=2, padding='valid', activation='relu'),\n",
    "            Conv1D(1, kernel_size=2, padding='valid', activation='sigmoid')\n",
    "        ]]\n",
    "    else:\n",
    "        encoder = [[\n",
    "            Input(shape=(original_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(intermediate_dim_2, activation='relu')\n",
    "        ]]\n",
    "\n",
    "        decoder = [[\n",
    "            Dense(intermediate_dim_2, activation='relu'),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(original_dim, activation='sigmoid')\n",
    "        ]]\n",
    "\n",
    "    model_obj = vae_tools.mmvae.MmVae(latent_dim, encoder, decoder, [original_dim], beta, reconstruction_loss_metrics = [vae_tools.mmvae.ReconstructionLoss.MSE], name='Vae')\n",
    "    vae = model_obj.get_model()\n",
    "    vae.compile(optimizer='rmsprop', loss=None)\n",
    "    # vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "\n",
    "    if X_set is None:\n",
    "        X_train, X_test, X_train_label, X_test_label = get_lidar_data(X_l, X_set_label_idx, original_dim, use_conv)\n",
    "    else:\n",
    "        X_train, X_test, X_train_label, X_test_label = X_set[\"X_train\"], X_set[\"X_test\"], X_set[\"X_train_label\"], X_set[\"X_test_label\"]\n",
    "        \n",
    "    \n",
    "    # Train\n",
    "    if train:\n",
    "        vae.fit(X_train,\n",
    "                shuffle=True,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                verbose = 0,\n",
    "                validation_data=(X_test, None))\n",
    "\n",
    "    return model_obj, use_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_camera(train = True, use_conv = False, X_set = None, epochs = 1000):\n",
    "\n",
    "    # Setup the VAE\n",
    "    batch_size = 256\n",
    "    # input image dimensions\n",
    "    image_rows_cols_chns = (64, 64, 2)\n",
    "    if use_conv:\n",
    "        # number of convolutional filters to use\n",
    "        filters = 32 # former 64\n",
    "        # convolution kernel size\n",
    "        num_conv = 3\n",
    "        # Other values\n",
    "        rows = image_rows_cols_chns[0]\n",
    "        rows_2 = int(rows/2)\n",
    "        cols = image_rows_cols_chns[1]\n",
    "        cols_2 = int(cols/2)\n",
    "        img_chns = image_rows_cols_chns[2]\n",
    "        if keras.backend.image_data_format() == 'channels_first':\n",
    "            original_img_size = (image_rows_cols_chns[2], image_rows_cols_chns[0], image_rows_cols_chns[1])\n",
    "            output_shape_reshape = (batch_size, filters, rows_2, cols_2)\n",
    "            output_shape_upsamp = (batch_size, filters, rows+1, cols+1)\n",
    "        else:\n",
    "            original_img_size = image_rows_cols_chns\n",
    "            output_shape_reshape = (batch_size, rows_2, cols_2, filters)\n",
    "            output_shape_upsamp = (batch_size, rows+1, cols+1, filters)\n",
    "    original_dim = np.prod(image_rows_cols_chns)\n",
    "    latent_dim = 2\n",
    "    intermediate_dim = 256 # former 128\n",
    "    intermediate_dim_2 = int(intermediate_dim / 2)\n",
    "    save_model = False\n",
    "    beta = 50.\n",
    "\n",
    "    if use_conv:\n",
    "        encoder = [[\n",
    "            Input(shape=original_img_size),\n",
    "            Conv2D(img_chns, kernel_size=(2, 2), padding='same', activation='relu'),\n",
    "            Conv2D(filters, kernel_size=(2, 2), padding='same', activation='relu', strides=(2, 2)),\n",
    "            Conv2D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1),\n",
    "            Conv2D(filters, kernel_size=num_conv, padding='same', activation='relu', strides=1),\n",
    "            Flatten(),\n",
    "            Dense(intermediate_dim, activation='relu')\n",
    "        ]]\n",
    "\n",
    "        decoder = [[\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(filters * rows_2 * cols_2, activation='relu'),\n",
    "            Reshape(output_shape_reshape[1:]),\n",
    "            Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu'),\n",
    "            Conv2DTranspose(filters, kernel_size=num_conv, padding='same', strides=1, activation='relu'),\n",
    "            Conv2DTranspose(filters, kernel_size=(3, 3), strides=(2, 2), padding='valid', activation='relu'),\n",
    "            Conv2D(img_chns, kernel_size=2, padding='valid', activation='sigmoid')\n",
    "        ]]\n",
    "\n",
    "    else:\n",
    "        encoder = [[\n",
    "            Input(shape=(original_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(intermediate_dim_2, activation='relu')\n",
    "        ]]\n",
    "\n",
    "        decoder = [[\n",
    "            Dense(intermediate_dim_2, activation='relu'),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(original_dim, activation='sigmoid')\n",
    "        ]]\n",
    "        \n",
    "    model_obj = vae_tools.mmvae.MmVae(latent_dim, encoder, decoder, [original_dim], beta, reconstruction_loss_metrics = [vae_tools.mmvae.ReconstructionLoss.MSE], name='Vae')\n",
    "    vae = model_obj.get_model()\n",
    "    vae.compile(optimizer='rmsprop', loss=None)\n",
    "    # vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "    \n",
    "    if X_set is None:\n",
    "        X_train, X_test, X_train_label, X_test_label = get_camera_data(X_c, X_set_label_idx, original_dim, use_conv)\n",
    "    else:\n",
    "        X_train, X_test, X_train_label, X_test_label = X_set[\"X_train\"], X_set[\"X_test\"], X_set[\"X_train_label\"], X_set[\"X_test_label\"]\n",
    "        \n",
    "    \n",
    "    # Train\n",
    "    if train:\n",
    "        vae.fit(X_train,\n",
    "                shuffle=True,\n",
    "                epochs = epochs,\n",
    "                batch_size = batch_size,\n",
    "                verbose = 0,\n",
    "                validation_data=(X_test, None))\n",
    "\n",
    "    \n",
    "    return model_obj, use_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum value: 6.7945444e-07\n",
      "Maximum value: 1.0\n",
      "Minimum value: 0.0\n",
      "Maximum value: 1.0\n",
      "Minimum value: 0.0\n",
      "Maximum value: 1.0\n",
      "Minimum value: 6.7945444e-07\n",
      "Maximum value: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d0de51c05e6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                                      use_conv_lidar, use_conv_camera)\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_lidar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conv_lidar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_lidar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_l_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_l_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_train_label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_test_label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel_camera\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_conv_camera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_camera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_c_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_c_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_train_label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X_test_label\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_test_label\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f3407f88e3ab>\u001b[0m in \u001b[0;36mvae_camera\u001b[0;34m(train, use_conv, X_set, epochs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 validation_data=(X_test, None))\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_lidar, use_conv_lidar = vae_lidar(train = False, use_conv = True)\n",
    "model_camera,use_conv_camera = vae_camera(train = False, use_conv = True)\n",
    "#model_lidar, use_conv_lidar = vae_lidar(train = True, use_conv = True, epochs = 10)\n",
    "#model_camera,use_conv_camera = vae_camera(train = True, use_conv = True, epochs = 10)\n",
    "X_l_train, X_l_test, X_c_train, X_c_test, X_train_label, X_test_label = get_data(X_c, X_l, X_set_label_idx, \n",
    "                                                                                     model_lidar.encoder_inputs_dim[0], model_camera.encoder_inputs_dim[0],\n",
    "                                                                                     use_conv_lidar, use_conv_camera)\n",
    "model_lidar, use_conv_lidar = vae_lidar(train = True, use_conv = True, epochs = 500, X_set = {\"X_train\": X_l_train, \"X_test\": X_l_test, \"X_train_label\": X_train_label, \"X_test_label\": X_test_label})\n",
    "model_camera,use_conv_camera = vae_camera(train = True, use_conv = True, epochs = 1000, X_set = {\"X_train\": X_c_train, \"X_test\": X_c_test, \"X_train_label\": X_train_label, \"X_test_label\": X_test_label})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_c_train[1].shape)\n",
    "image =X_c_train[1]\n",
    "image = (255. - np.concatenate((image, np.ones((image.shape[1],image.shape[0],1))), axis=2) * 255.) / 255.\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ model_lidar.get_encoder_mean([model_lidar.encoder[0][0]]).predict(X_l_train),\n",
    "        model_camera.get_encoder_mean([model_camera.encoder[0][0]]).predict(X_c_train)]\n",
    "data_test = [ model_lidar.get_encoder_mean([model_lidar.encoder[0][0]]).predict(X_l_test),\n",
    "        model_camera.get_encoder_mean([model_camera.encoder[0][0]]).predict(X_c_test)]\n",
    "\n",
    "\n",
    "idx = 1\n",
    "print(data[idx].shape)\n",
    "plt.scatter(data[idx][:,0], data[idx][:,1], c = X_train_label)\n",
    "plt.show()\n",
    "print(data[idx][:,0])\n",
    "print(model_camera.get_encoder_logvar([model_camera.encoder[0][0]]).predict(X_c_train[[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi modal training of proprio and voxel\n",
    "data = [ model_lidar.get_encoder_mean([model_lidar.encoder[0][0]]).predict(X_l_train),\n",
    "        model_camera.get_encoder_mean([model_camera.encoder[0][0]]).predict(X_c_train)]\n",
    "data_test = [ model_lidar.get_encoder_mean([model_lidar.encoder[0][0]]).predict(X_l_test),\n",
    "        model_camera.get_encoder_mean([model_camera.encoder[0][0]]).predict(X_c_test)]\n",
    "model_name = \"lidar_camera_vae\"\n",
    "store_model = True # Train and store the model if \"True\" or loads existing models if \"False\"\n",
    "store_model_overwrite = False # Overwrite model if it exists\n",
    "store_model_prefix = \"models/lidar_camera\" # prefix for loading and storing\n",
    "if store_model:\n",
    "\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    z_dim = 2\n",
    "    beta_norm = 0.01\n",
    "    original_dim = 2\n",
    "    intermediate_dim = 64\n",
    "    intermediate_dim_2 = int(intermediate_dim / 2)\n",
    "    epsilon_std = 1.\n",
    "    beta = 0.125\n",
    "\n",
    "    encoder_pv = [[\n",
    "            Input(shape=(original_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(intermediate_dim_2, activation='relu')\n",
    "        ],[\n",
    "            Input(shape=(original_dim,)),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(intermediate_dim_2, activation='relu')\n",
    "        ]]\n",
    "    decoder_pv = [[\n",
    "            Dense(intermediate_dim_2, activation='relu'),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(original_dim, activation='sigmoid')\n",
    "        ],[\n",
    "            Dense(intermediate_dim_2, activation='relu'),\n",
    "            Dense(intermediate_dim, activation='relu'),\n",
    "            Dense(original_dim, activation='sigmoid')\n",
    "        ]]\n",
    "\n",
    "    vae_obj = vae_tools.mmvae.MmVae(z_dim, encoder_pv, decoder_pv, [2, 2],\n",
    "                    beta_norm, beta_is_normalized = True, reconstruction_loss_metrics = [vae_tools.mmvae.ReconstructionLoss.MSE], name=model_name)\n",
    "\n",
    "    vae = vae_obj.get_model()\n",
    "    vae.compile(optimizer='adam', loss=None)\n",
    "    #vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "    # Store the losses, encodings, and decodings to tensorboard\n",
    "    losses_cb = vae_tools.callbacks.Losses(data = data_test)\n",
    "    # Train\n",
    "    vae.fit(data, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=None, verbose = 1, callbacks=[losses_cb])\n",
    "    # Show the losses\n",
    "    try:\n",
    "        vae_tools.viz.plot_losses(losses_cb, plot_elbo = True)\n",
    "    except:\n",
    "        pass\n",
    "   # Get the models\n",
    "    encoder_input = [encoder_pv[0][0], encoder_pv[1][0]]\n",
    "    encoder_mean = [vae_obj.get_encoder_mean([encoder_input[0]]),\n",
    "                                 vae_obj.get_encoder_mean([encoder_input[1]]),\n",
    "                                 vae_obj.get_encoder_mean(encoder_input)]\n",
    "    encoder_logvar = [vae_obj.get_encoder_logvar([encoder_input[0]]),\n",
    "                                   vae_obj.get_encoder_logvar([encoder_input[1]]),\n",
    "                                   vae_obj.get_encoder_logvar(encoder_input)]\n",
    "    \n",
    "    # Store model\n",
    "    #vae_obj.store_model(name = store_model_prefix + \"_\" + model_name + \"_dec\", model = vae_obj.get_decoder(), overwrite = store_model_overwrite)\n",
    "    vae_obj.store_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_mean_ProprioVoxel\",\n",
    "                                 model_inputs = encoder_input,\n",
    "                                 get_model_callback = vae_obj.get_encoder_mean, overwrite = store_model_overwrite)\n",
    "    vae_obj.store_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_logvar_ProprioVoxel\",\n",
    "                                 model_inputs = encoder_input,\n",
    "                                 get_model_callback = vae_obj.get_encoder_logvar, overwrite = store_model_overwrite)\n",
    "else:\n",
    "    # Load the models\n",
    "    #decoder_mean = MmVae.load_model(name = store_model_prefix + \"_\" + model_name + \"_dec\")\n",
    "    encoder_mean, _ =  vae_tools.mmvae.MmVae.load_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_mean_ProprioVoxel\",\n",
    "                                             num_elements = 2)\n",
    "    encoder_logvar, _ =  vae_tools.mmvae.MmVae.load_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_logvar_ProprioVoxel\",\n",
    "                                               num_elements = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model_lidar, use_conv_lidar = vae_lidar(train = False)\n",
    "model_camera,use_conv_camera = vae_camera(train = False)\n",
    "X_l_train, X_l_test, X_c_train, X_c_test, X_train_label, X_test_label = get_data(X_c, X_l, X_set_label_idx, \n",
    "                                                                                     model_lidar.encoder_inputs_dim[0], model_camera.encoder_inputs_dim[0],\n",
    "                                                                                     use_conv_lidar, use_conv_camera)\n",
    "# Multi modal training of proprio and voxel\n",
    "data = [X_l_train, X_c_train]\n",
    "data_test = [X_l_test, X_c_test]\n",
    "model_name = \"lidar_camera_vae\"\n",
    "store_model = True # Train and store the model if \"True\" or loads existing models if \"False\"\n",
    "store_model_overwrite = False # Overwrite model if it exists\n",
    "pin_weights = False\n",
    "store_model_prefix = \"models/lidar_camera\" # prefix for loading and storing\n",
    "if store_model:\n",
    "\n",
    "    batch_size = 1024\n",
    "    epochs = 100\n",
    "    z_dim = 2\n",
    "    beta_norm = 0.01\n",
    "\n",
    "    # Pin the weights\n",
    "    if pin_weights:\n",
    "        for layer in model_lidar.encoder[0]:\n",
    "            layer.trainable = False\n",
    "        for layer in model_lidar.decoder[0][1:]:\n",
    "            layer.trainable = False\n",
    "        for layer in model_camera.encoder[0]:\n",
    "            layer.trainable = False\n",
    "        for layer in model_camera.decoder[0][1:]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    encoder_pv = [model_lidar.encoder[0], model_camera.encoder[0]]\n",
    "    decoder_pv = [model_lidar.decoder[0], model_camera.decoder[0]]\n",
    "\n",
    "    #vae_obj = vae_tools.mmvae.MmVae(z_dim, encoder_pv, decoder_pv, [model_lidar.encoder_inputs_dim[0], model_camera.encoder_inputs_dim[0]],\n",
    "    #                beta_norm, beta_is_normalized = True, reconstruction_loss_metrics = [vae_tools.mmvae.ReconstructionLoss.MSE], name=model_name)\n",
    "    vae_obj = vae_tools.mmvae.MmVae(z_dim, encoder_pv, decoder_pv, [model_lidar.encoder_inputs_dim[0] * 4., model_lidar.encoder_inputs_dim[0]],\n",
    "                    beta_norm, beta_is_normalized = True, reconstruction_loss_metrics = [vae_tools.mmvae.ReconstructionLoss.MSE], name=model_name)\n",
    "\n",
    "    vae = vae_obj.get_model()\n",
    "    vae.compile(optimizer='adam', loss=None)\n",
    "    #vae_tools.viz.plot_model(vae, file = 'myVAE', print_svg = False, verbose = True)\n",
    "    # Store the losses, encodings, and decodings to tensorboard\n",
    "    losses_cb = vae_tools.callbacks.Losses(data = data_test)\n",
    "    # Train\n",
    "    # vae.fit(data, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=None, verbose = 1, callbacks=[losses_cb])\n",
    "    vae.fit(data, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=None, verbose = 0)\n",
    "    # Show the losses\n",
    "    try:\n",
    "        vae_tools.viz.plot_losses(losses_cb, plot_elbo = True)\n",
    "    except:\n",
    "        pass\n",
    "   # Get the models\n",
    "    encoder_input = [encoder_pv[0][0], encoder_pv[1][0]]\n",
    "    encoder_mean = [vae_obj.get_encoder_mean([encoder_input[0]]),\n",
    "                                 vae_obj.get_encoder_mean([encoder_input[1]]),\n",
    "                                 vae_obj.get_encoder_mean(encoder_input)]\n",
    "    encoder_logvar = [vae_obj.get_encoder_logvar([encoder_input[0]]),\n",
    "                                   vae_obj.get_encoder_logvar([encoder_input[1]]),\n",
    "                                   vae_obj.get_encoder_logvar(encoder_input)]\n",
    "    \n",
    "    # Store model\n",
    "    #vae_obj.store_model(name = store_model_prefix + \"_\" + model_name + \"_dec\", model = vae_obj.get_decoder(), overwrite = store_model_overwrite)\n",
    "    vae_obj.store_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_mean_ProprioVoxel\",\n",
    "                                 model_inputs = encoder_input,\n",
    "                                 get_model_callback = vae_obj.get_encoder_mean, overwrite = store_model_overwrite)\n",
    "    vae_obj.store_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_logvar_ProprioVoxel\",\n",
    "                                 model_inputs = encoder_input,\n",
    "                                 get_model_callback = vae_obj.get_encoder_logvar, overwrite = store_model_overwrite)\n",
    "else:\n",
    "    # Load the models\n",
    "    #decoder_mean = MmVae.load_model(name = store_model_prefix + \"_\" + model_name + \"_dec\")\n",
    "    encoder_mean, _ =  vae_tools.mmvae.MmVae.load_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_mean_ProprioVoxel\",\n",
    "                                             num_elements = 2)\n",
    "    encoder_logvar, _ =  vae_tools.mmvae.MmVae.load_model_powerset(prefix = store_model_prefix + \"_\" + model_name + \"_enc_logvar_ProprioVoxel\",\n",
    "                                               num_elements = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vae_tools\n",
    "from importlib import reload\n",
    "reload(vae_tools.viz)\n",
    "mean_encoded = encoder_mean[0].predict(data[0])\n",
    "vae_tools.viz.plot_embedding(mean_encoded, X_train_label, figsize=(6, 6), colormap=None, show_ticks = True)\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"lidar\")\n",
    "\n",
    "mean_encoded = encoder_mean[1].predict(data[1])\n",
    "vae_tools.viz.plot_embedding(mean_encoded, X_train_label, figsize=(6, 6), colormap=None, show_ticks = True)\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"camera\")\n",
    "\n",
    "# Plot the embedding proprio and voxel\n",
    "mean_encoded = encoder_mean[2].predict(data)\n",
    "#var_encoded = np.sum(np.exp(encoder_logvar[2].predict(data, batch_size=batch_size)),axis=-1) # trace of co-variance\n",
    "#vae_tools.viz.plot_embedding(mean_encoded, var_encoded, colormap=\"rainbow\", figsize=(6, 6))\n",
    "#plt.colorbar()\n",
    "vae_tools.viz.plot_embedding(mean_encoded, X_train_label, figsize=(6, 6), colormap=None, show_ticks = True)\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"lidar/camera\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 14\n",
    "print(X_l.shape)\n",
    "plt.plot(X_l[idx])\n",
    "#plt.plot(data[0][idx])\n",
    "plt.show()\n",
    "image = data[1][idx]\n",
    "if len(image.shape) == 1:\n",
    "    image = np.matlib.reshape(image, (64,64,2))\n",
    "image = (255. - np.concatenate((image, np.ones((image.shape[1],image.shape[0],1))), axis=2) * 255.) / 255.\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_idx = 0 # 0:lidar, 1: camera\n",
    "X_train = data[model_idx]\n",
    "encoder = encoder_mean[model_idx]\n",
    "encoder_lv = encoder_logvar[model_idx]\n",
    "if model_idx == 0:\n",
    "    decoder = vae_obj.get_decoder(decoder_output_list = [model_lidar.decoder[0][-1]])\n",
    "else:\n",
    "    decoder = vae_obj.get_decoder(decoder_output_list = [model_camera.decoder[0][-1]])\n",
    "\n",
    "# Vizualization\n",
    "# Encode samples to get the min and max values in latent space\n",
    "X_encoded = encoder.predict(X_train, batch_size=batch_size)\n",
    "\n",
    "# Plot in- and output signals\n",
    "if model_idx == 0:\n",
    "    viz.lidar_in_out_2(X_train, 16, encoder, decoder)\n",
    "\n",
    "# display a 2D manifold\n",
    "nx = 50\n",
    "ny = 50\n",
    "\n",
    "# Chose linearly spaced coordinates according to the above figure\n",
    "grid_x = (np.linspace(np.min(X_encoded[:, 0]), np.max(X_encoded[:, 0]), nx))\n",
    "grid_y = (np.linspace(np.min(X_encoded[:, 1]), np.max(X_encoded[:, 1]), ny))\n",
    "grid_x = norm.ppf(np.linspace(0.001, 0.999, nx))\n",
    "grid_y = norm.ppf(np.linspace(0.001, 0.999, ny))\n",
    "grid_x = np.linspace(np.amin(grid_x), np.amax(grid_x), nx)\n",
    "grid_y = np.linspace(np.amin(grid_y), np.amax(grid_y), ny)\n",
    "\n",
    "# Project input to latent space\n",
    "viz.scatter_encoder_2(X_train, X_train_label, grid_x, grid_y, encoder, figsize=(15, 15), dpi=150)\n",
    "\n",
    "# Plot the resampled inputs\n",
    "if model_idx == 0:\n",
    "    figures, z_reencoded_mean, z_reencoded_std = viz.get_lidar_dec_enc_samples_2(grid_x, grid_y, encoder, encoder_lv, decoder, Dz = 2)\n",
    "    plt.show()\n",
    "else:\n",
    "    figure, z_reencoded_mean, z_reencoded_std = viz.get_image_dec_enc_samples_2(grid_x, grid_y, encoder, encoder_lv, decoder, Dz = 2, image_size = image_rows_cols_chns)\n",
    "    plt.figure(figsize=(15, 15), dpi=96)\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the resampled std deviations\n",
    "X, Y = np.meshgrid(np.arange(0,len(grid_x)), np.arange(0,len(grid_y)))\n",
    "plt.pcolor(X, Y, z_reencoded_std, cmap='coolwarm', vmin=z_reencoded_std.min(), vmax=z_reencoded_std.max())\n",
    "plt.colorbar()\n",
    "plt.axis(\"image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
